{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pymongo\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# Wikipedia URL for highest-grossing films\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n",
    "\n",
    "# Headers for requests\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_wikipedia_page(url):\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the page\")\n",
    "        return None\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "def clean_numeric(value):\n",
    "    return int(re.sub(r'\\D', '', value))  # Remove all non-numeric characters\n",
    "\n",
    "def extract_film_data():\n",
    "    soup = get_wikipedia_page(URL)\n",
    "    \n",
    "    table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "    rows = table.find_all(\"tr\")  # Skipping header row\n",
    "    films = []\n",
    "    \n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        title_col = row.find(\"th\")\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "        \n",
    "        rank = clean_numeric(cols[0].get_text(strip=True))\n",
    "        peak = clean_numeric(cols[1].get_text(strip=True))\n",
    "        revenue = clean_numeric(cols[2].get_text(strip=True))\n",
    "        year = clean_numeric(cols[3].get_text(strip=True))\n",
    "        \n",
    "        film_page_link = title_col.find(\"a\")\n",
    "        title = title_col.get_text(strip=True)\n",
    "        film_url = \"https://en.wikipedia.org\" + film_page_link[\"href\"] if film_page_link else None\n",
    "        \n",
    "        print(rank, peak, title, revenue, year)\n",
    "        print(film_url)\n",
    "      \n",
    "        if film_url:\n",
    "            director, country = get_film_details(film_url)\n",
    "        else:\n",
    "            director, country = \"Unknown\", \"Unknown\"\n",
    "        \n",
    "        films.append({\"title\": title, \"release_year\": year, \"director\": director, \"revenue\": revenue, \"country\": country})\n",
    "        time.sleep(0.1)  # Avoid hitting Wikipedia too frequently\n",
    "    \n",
    "    return films\n",
    "\n",
    "def get_film_details(film_url):\n",
    "    soup = get_wikipedia_page(film_url)\n",
    "    if soup is None:\n",
    "        return \"Unknown\", \"Unknown\"\n",
    "    \n",
    "    info_box = soup.find(\"table\", {\"class\": \"infobox\"})\n",
    "    director, country = \"Unknown\", \"Unknown\"\n",
    "    \n",
    "    if info_box:\n",
    "        rows = info_box.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            header = row.find(\"th\")\n",
    "            if not header:\n",
    "                continue\n",
    "            \n",
    "            if \"Directed by\" in header.text:\n",
    "                director_data = row.find(\"td\")\n",
    "                if director_data:\n",
    "                    directors = [li.get_text(strip=True) for li in director_data.find_all(\"li\")]\n",
    "                    director = directors[0] if directors else director_data.get_text(strip=True)\n",
    "                \n",
    "            if \"Country\" in header.text or \"Countries\" in header.text:\n",
    "                country_data = row.find(\"td\")\n",
    "                if country_data:\n",
    "                    countries = [li.get_text(strip=True) for li in country_data.find_all(\"li\")]\n",
    "                    country = countries[0] if countries else country_data.get_text(strip=True)\n",
    "    \n",
    "    return director, country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_json(films):\n",
    "    with open(\"films_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(films, f, indent=4)\n",
    "    print(\"Data saved to films_data.json for testing.\")\n",
    "\n",
    "def load_data_from_json():\n",
    "    if os.path.exists(\"films_data.json\"):\n",
    "        with open(\"films_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            films = json.load(f)\n",
    "        print(\"Data loaded from films_data.json.\")\n",
    "        return films\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_URI = \"mongodb+srv://bodashkaxdgg:uL9f1Yj8frIY7zua@cluster0.idr3e.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "class AtlasClient:\n",
    "    def __init__(self, atlas_uri, dbname):\n",
    "        self.mongodb_client = pymongo.MongoClient(atlas_uri)\n",
    "        self.database = self.mongodb_client[dbname]\n",
    "\n",
    "    def ping(self):\n",
    "        self.mongodb_client.admin.command(\"ping\")\n",
    "\n",
    "    def get_collection(self, collection_name):\n",
    "        return self.database[collection_name]\n",
    "\n",
    "    def insert_many(self, collection_name, data):\n",
    "        collection = self.get_collection(collection_name)\n",
    "        collection.insert_many(data)\n",
    "        print(\"Data inserted successfully.\")\n",
    "\n",
    "    def delete_all(self, collection_name):\n",
    "        collection = self.get_collection(collection_name)\n",
    "        collection.delete_many({})\n",
    "        print(\"All documents deleted.\")\n",
    "\n",
    "    def find(self, collection_name, filter={}, limit=0):\n",
    "        collection = self.get_collection(collection_name)\n",
    "        return list(collection.find(filter, limit=limit))\n",
    "    \n",
    "atlas_client = AtlasClient(ATLAS_URI, \"movies_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from films_data.json.\n",
      "Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    films = load_data_from_json()\n",
    "    if films is None:\n",
    "        films = extract_film_data()\n",
    "        save_data_to_json(films)\n",
    "    atlas_client.insert_many(\"highest_grossing_films\", films)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
